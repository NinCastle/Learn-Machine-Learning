{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 학습 기술들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[아래 내용에 대한 링크](https://ratsgo.github.io/deep%20learning/2017/04/22/NNtricks/)\n",
    "<br>\n",
    "아래 내용은 상위 링크를 내용 정리한것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 학습의 일반적 절차\n",
    "<br>\n",
    "딥러닝의 일반적 절차는 다음과 같다.\n",
    "1. 적절한 네트워크 선택<br>\n",
    "    1) 구조(structure): single words VS Bag of Words?, etc.<br>\n",
    "[Bag of Words Link](http://darkpgmr.tistory.com/125)<br>\n",
    "    2) 비선형성(nonlinearity) 획득 방법: ReLU VS Tanh, etc\n",
    "2. Gradient Chack:\n",
    "    네트워크를 구축했는데 그래디언트 계산이 혹시 잘못될 염려가 있으므로 잘됐는지 체크\n",
    "3. 학습 파라미터 초기화: 초기화 방법에도 여러가지 방법.? 있음 적절히 선택\n",
    "4. 학습 파라미터 최적화: Stochastic Gradient VS Adam, etc\n",
    "5. 과적합 방지: Dropout VS regularize, etc.<br>\n",
    "<br>\n",
    "\n",
    "## 비선형성 획득: 활성함수\n",
    "<br>\n",
    "뉴럴네트워크의 개별 뉴런에 들어오는 입력신호의 총합을 출력신호로 변환하는 함수를 활성화함수(Activation Function)라고 합니다.<br> \n",
    "활성화함수 유무는 초창기 모델인 퍼센트론(perceptron)과 뉴럴네트워크의 유일한 차이점.<br>\n",
    "Activation Function은 대개 비선형 함수(Non-Linear Function)을 쓴다.<br>\n",
    "Activation Function으로 선형함수를 쓰면 안된다. 왜냐?<br>\n",
    "<br>\n",
    "'밑바닥부터 시작하는 딥러닝' 중에서....<br>\n",
    "<br>\n",
    "선형 함수인$h(x) = cx$를 Activation Fanction으로 사용한 3층 네트워크를 떠올려 보세요, 이를 식으로 나타내면 $y(x) = h(h(h(x)))$가 됩니다. 이는 실은 $y(x) = ax$와 똑같은 식 입니다. $a = c^3$이라고 하면 끝이죠. 즉, 은닉층이 없는 네트워크로 표현할 수 있습니다. 뉴럴네트워크에서 층을 쌓는 혜택을 얻고 싶다면 활성화함수로는 반드시 비선형 함수를 사용해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 뉴럴 네트워크 활성화 함수 몇 가지를 살펴본다.<br>\n",
    "<br>\n",
    "### 시그모이드\n",
    "<br>\n",
    "로지스틱 함수로도 불린다. 원래 수식 및 미분식은 아래와 같다.<br>\n",
    "<br>\n",
    "$$\n",
    "\\sigma(x) = \\frac {1}{1+e^{-x}}\n",
    "$$\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x)(1-\\sigma(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시그모이드 함수의 범위는 [0, 1]이며, 그래프의 모양은 아래와 같다.<br>\n",
    "![Sigmoid_function](http://i.imgur.com/HpSpWal.png)\n",
    "<br>\n",
    "시그모이드 함수를 1차 미분한 그래프는 아래와 같다.\n",
    "<br>\n",
    "![sigmoid_1차 미분](http://i.imgur.com/WpKD6kW.png)\n",
    "<br>\n",
    "시그모이드에 미분을 하는 의미<br>\n",
    "[시그모이드 미분 공식 링크](http://taewan.kim/post/sigmoid_diff/)\n",
    "$$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시그모이드 함수는 개별 뉴런이 내 뱉어 주는 값을 S자 커브형태로 자연스럽게 활성화를 해주기 떄문에 인기있다.<br>\n",
    "다만, 입력값이 -5보다 작거나 5보다 클 경우에는 그래디언트 값이 지나치게 작아지고<br>(=이렇게 되면 학습이 잘 안된다.)<br>\n",
    ", exp 연산이 다소 무겁다(=학습이 더디다.)는 단점이 있다.<br>\n",
    "<br>\n",
    "아울러 $\\sigma(x)$의 범위는 $[0, 1]$로써 0이상의 값을 지닌다는 문제가 있다.<br> \n",
    "이게 단점이 되는 이유는 바로 학습 속도와 관련이 있다.<br>\n",
    "예를 들어, <br>\n",
    "아래와 같은 뉴런이 있고 활성화 함수 $f$가 시그모이드라고 가정할때,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "![Sigmoid](http://i.imgur.com/euw7qQu.png)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_0, x_1, x_2$는 모두 0 이상의 값을 갖는다.<br> \n",
    "이들은 직전 층에서 시그모이드 함수에 의해 그 값이 양수로 활성화 됐기 때문이다.<br>\n",
    "여기에서 역전파시 최종 Loss에서 출발해 시그모이드 적용 직전의 $w_ix_i + b$ 각각에 들오어고 있는 그래디언트를 $\\delta$라고 한다.\n",
    "즉,\n",
    "$$\n",
    "\\sum_i w_ix_i + b=\\delta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면 $w_i$의 그래디언트는 아래와 같다.\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_i} = x_i \\times \\delta\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
