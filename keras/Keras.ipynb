{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is artificial neural network:<br>\n",
    "인공신경망(人工神經網, 영어: artificial neural network, ANN)은 기계학습과 인지과학에서 생물학의 신경망(동물의 중추신경계중 특히 뇌)에서 영감을 얻은 통계학적 학습 알고리즘이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![neural](http://study.zumst.com/upload/00-D22-91-12-05/1_2_a_%EB%89%B4%EB%9F%B0%EC%9D%98%20%EA%B5%AC%EC%A1%B0.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions\n",
    "\n",
    "* Neural networks account for interaction really well\n",
    "* Deep learning uses especially powerful neural networks\n",
    "    * Text\n",
    "    * imge 등등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we learn?\n",
    "* First focus on conceptual knowledge\n",
    "    - Debug and tune deep learning models on conventional prediction problems\n",
    "    - Lay the foundation for progressing towards modern applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions in neural network\n",
    "\n",
    "All network has Inpust Layer, Hidden Layer and Output Layer\n",
    "\n",
    "\n",
    "![neural network](https://cdn-images-1.medium.com/max/479/1*QVIyc5HnGDWTNX3m-nIm9w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If layer has more hidden layers, Ability is increase (more and more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bank Transactions example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make predictions based on:\n",
    "    * Number of children\n",
    "    * Number of exisiting accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forwoard Propagation\n",
    "\n",
    "|input layer|hidden layer|output layer|\n",
    "|----|----|----|\n",
    "|x|w|y|\n",
    "$$\n",
    "xW=y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "leyer image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forword propagation\n",
    "* Multiply - add process\n",
    "* Dot product\n",
    "* Foward propagation for one data point at a time\n",
    "* Output is the prediction for that data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 1]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_data = np.array([2, 3]) # input value\n",
    "\n",
    "weights = {'node_0' : np.array([1, 1]),\n",
    "          'node_1' : np.array([-1, 1]),\n",
    "          'output': np.array([2, -1])} # weights for input_data\n",
    "\n",
    "node_0_value = (input_data * weights['node_0']).sum() # input 에 Weights 곱하여 더한다.\n",
    "node_1_value = (input_data * weights['node_1']).sum()\n",
    "\n",
    "hidden_layer_values = np.array([node_0_value, node_1_value]) \n",
    "# 각각의 input값들을 각각의 가중치와 곱한뒤 나온 값들을 더한것을 hidden_layer_values 변수에 삽입\n",
    "\n",
    "print(hidden_layer_values)\n",
    "\n",
    "output = (hidden_layer_values * weights['output']).sum() # hidden_layer_value와 output값을 곱하여 두 값을 더했다.\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation funtions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear VS Nonlinear Functions\n",
    "\n",
    "선형, 비선형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "앞선 Layer 관계를 통해 확인가능\n",
    "\n",
    "|input|Hidden Layer|output|\n",
    "|---|---|---|\n",
    "|2, 3|tanh(2+3), tanh(-2+3)|9|<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradient Problem\n",
    " [Gradient Vanising에 대한 문제 링크](http://ydseo.tistory.com/41)<br>\n",
    " <br>\n",
    "ReLU(Rectified Linear Activation)에 대해 배우기전에 Vanishing gradiendt problem에 대해 알아 둘 필요가 있다.<br>\n",
    "Vanising Gradient Problem(기울기 값이 사라지는 문제)는<br> \n",
    "인공신경망을 기울기 값을 베이스로 하는 method(backpropagation)로<br> \n",
    "학습시키려고 할때 발생되는 어려움이다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Vanishing Gradient Problem은 Neural Network의 군본적인 문제점이 아니다.<br>\n",
    "이것은 특정한 Activation function을 통해서 기울기 베이스의 학습 방법을 사용할때 문제가 된다.<br>\n",
    "아래는 문제의 예시이다.<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[문제]<br>\n",
    "Gradient 기반의 방법은 parameter value<br>($input \\times W$의 output value 즉 layers valuse들)의<br> \n",
    "작은 변화가 network output에 얼마나 영향을 미칠지를 이해하는 기반으로 parameter value를 학습시킨다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약<br>\n",
    "**Parameter value의 변화가 Network's output의 매우 작은 변화를 야기한다면**,<br> \n",
    "**네트워크는 Parameter를 효과적으로 학습시킬 수 없게 되는데 이것이 문제다.**<br>\n",
    "<br>\n",
    "즉, Gradient라는 것이 결국 미분값 즉 변화량을 의미하는데, 이 변화량이 매우작다면,<br> \n",
    "network를 효과적으로 학습시키지 못하고,<br> \n",
    "error rate가 미쳐 다 낮아지기 전에 수렴(종료)해 버리는 문제가 발생한다는 것.<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 상황이 바로 Vanishing gradient problem으로 발생하는 문제인데,<br> \n",
    "이 문제로 초기 레이어에 각각의 parameter들에 대한 network's output의 gradient는 극도로 작아지게 된다.<br> \n",
    "다르게 말하자면, 초기 레이어에서 parameter value에 대한 큰 변화가 발생해도 output에 큰 영향을 주지 못한다.<br>\n",
    "그렇다면, 언제, 왜 이러한 문제가 발생하는지 아래에서 살펴본다.<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[원인]<br>\n",
    "Vanishing gradient problem은 activation function을 선택하는 문제에 의존적으로 일어난다.<br>\n",
    "<span style=\"color:red\">Sigmoid</span>나, <span style=\"color:green\">Tanh</span>등 요즘 많이 사용하는 Activation function들은 매우 비선형적 방식으로 두 방식의 input을 매우 작은 output range로 짓이겨 넣는다('squash')<br>\n",
    "<br>\n",
    "예를 들어, <span style=\"color:red\">Sigmoid</span>는 실수 범위의 수를 [0.1] 맵핑한다.<br>\n",
    "그 결과로 매우 넓은 input space에서는 큰 변화가 있다고 하더라도,<br> \n",
    "output에는 작은 변화를 보이게 된다.<br>\n",
    "Gradient(기울기)가 작기 때문이다.<br>\n",
    "<br>\n",
    "![sigmoid & Relu](http://cfile4.uf.tistory.com/image/22293C50579F7BBF1331FA)\n",
    "<br>\n",
    "이러한 현상은 비선형성을 여러개 레이어로 쌓을 때 더욱 악화된다.<br>\n",
    "<br>\n",
    "예를 들어, 첫 레이어에서 넓은 input region을 작은 output region으로 맵핑하고,<br> \n",
    "그것이 2차 3차 레이어로 갈수록 더 심각하게 작은 region으로 맵핑되는 경우이다.<br>\n",
    "그 결과로 첫 레이어 input에 대해 매우 큰 변화가 있다고 하더라도 output을 크게 변화시키지 못하게 된다.<br>\n",
    "<br>\n",
    "이러한 문제를 해결하기 위해 짓이겨 넣는 식('squashing')의 특징을 갖지 않는 activation function즉,<br> \n",
    "ReLU(Rectified Linear Unit-max(0,x))를 선택한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU(Rectified Linear Activation)\n",
    "[ReLU관한 링크](http://mongxmongx2.tistory.com/25),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU를 사용하기 이전에는 activation function으로 sigmoid function을 사용했다.<br> \n",
    "sigmoid function이 연속이여서 미분가능한점과 0과 1사이의 값을 가진다는 점<br>\n",
    "그리고 0에서 1로 변하는 점이 가파르기 때문에 사용해왔다.<br> \n",
    "그러나 기존에 사용하던 Simgoid fucntion을 ReLu가 대체하게 된 이유 중 가장 큰 것이<br> <span style=\"color:red\">Gradient Vanishing</span> 문제이다.<br>\n",
    "Simgoid function은 0에서 1사이의 값을 가지는데 gradient descent를 사용해 Backpropagation 수행시 layer를 지나면서 gradient를 계속 곱하므로 gradient는 <span style=\"color:red\">0으로 수렴</span>하게 된다.<br>\n",
    "따라서 layer가 많아지면 잘 작동하지 않게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**따라서 이러한 문제를 해결하기위해 ReLu를 새로운 activation function을 사용한다. ReLu는 입력값이 0보다 작으면 0이고 0보다 크면 입력값 그대로를 내보낸다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ReLU](http://cfile9.uf.tistory.com/image/246B094F57F226C0366860)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tanh(쌍곡선 함수):\n",
    "수학에서 쌍곡선함수(双曲線函數)는 일반적인 삼각함수와 유사한 성질을 갖는 함수로 삼각함수가 단위원 그래프를 매개변수로 표시할 때 나오는 것처럼, 표준쌍곡선을 매개변수로 표시할 때 나온다.\n",
    "\n",
    "tanh를 사용하는 이유:\n",
    "sigmod쓰지 않고 tanh를 쓰는 이유는 수식으로 쓰자면 0.5 (tanh (x)+1)=sigmoid (2x) 라 tanh (x)는 sigmoid (x)보다 두배 빨라서(추측)\n",
    "\n",
    "그렇다면 sigmod를 왜 사용하나:\n",
    "sigmoid는 logistic classification에서 어디에 속하는지 분류를 하기 위해 사용했다. 일정 값을 넘어야 성공내지는 참(True)이 될 수 있기 때문에 Activation function이라고도 불렀다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "![Activation Function](http://cfile28.uf.tistory.com/image/253F5947579F7BC32B036E)\n",
    "<br>\n",
    "다양한 activaion 함수들.<br>\n",
    "<br>\n",
    "**tanh** - sigmoid 함수를 재활용하기 위한 함수. sigmoid의 범위를 -1에서 1로 넓혔다.<br>\n",
    "**ReLU** - max(0,x)처럼 음수에 대해서만 0처리하는 함수<br>\n",
    "**Leaky ReLU** - ReLU 함수의 변형으로 음수에 대해 1/10로 값을 줄여서 사용하는 함수.<br>\n",
    "**ELU** - ReLU를 0이 아닌 다른 값을 기준으로 사용하는 함수<br>\n",
    "**Maxout** - 두 개의 W와 b 중에서 큰 값이 나온 것을 사용하는 함수<br>  \n",
    "<br>\n",
    "![Activation Functions on CIFAR-10](http://cfile30.uf.tistory.com/image/275DB947579F7BC410EC4C)\n",
    "<br>\n",
    "Activation 함수의 성능을 비교하고 있다.<br> \n",
    "sigmoid함수가 n/c라고 결과 값이 나온것은 결과를 낼 수 없기 때문.<br>\n",
    "sigmoid는 매우 비효율적인 Activation Function임이 증명되는 순간.<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본론으로 돌아와 ReLu를 이용하여 어떤 특징을 잡고, 이로부터 A인지 B인지 에 대해 한 구분을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.99995084645\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "input_data = np.array([-1, 2])\n",
    "\n",
    "weights = {'node_0' : np.array([3, 3]),\n",
    "          'node_1' : np.array([1, 5]),\n",
    "          'output' : np.array([2, -1])}\n",
    "\n",
    "node_0_input = (input_data * weights['node_0'].sum())\n",
    "\n",
    "node_0_output = np.tanh(node_0_input) # ReLu 대신 Tanh를 사용\n",
    "\n",
    "node_1_input = (input_data * weights['node_1'].sum())\n",
    "\n",
    "node_1_output = np.tanh(node_1_input)\n",
    "\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "52\n",
      "364\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_data = np.array([3, 5])\n",
    "\n",
    "def relu(input):\n",
    "    relu_value = max(0, input)\n",
    "    return relu_value\n",
    "\n",
    "weights = {'node_0':np.array([2, 4]),\n",
    "          'node_1': np.array([4, -5]),\n",
    "          'node_0_output' : np.array([-1, 2]),\n",
    "          'node_1_output' : np.array([2, 1]),# 왜 ([1, 2])는 안되는가?\n",
    "          'output' : np.array([-3, 7])}\n",
    "\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "print(node_0_output)\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "# print(node_1_output)\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "node_0_output2 = (hidden_layer_outputs * weights['node_0_output']).sum()\n",
    "node_0_output3 = relu(node_0_output2)\n",
    "# print(node_0_output3)\n",
    "\n",
    "node_1_output2 = (hidden_layer_outputs * weights['node_1_output']).sum()\n",
    "node_1_output3 = relu(node_1_output2)\n",
    "print(node_1_output3)\n",
    "\n",
    "hidden_layer_outputs2 = np.array([node_0_output3, node_1_output3])\n",
    "\n",
    "output = (hidden_layer_outputs2 * weights['output']).sum()\n",
    "\n",
    "print(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "26*7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation Learning(표현학습)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deep networks internally build representations of patterns in the data\n",
    "* Partially replace the need for feature enginnering\n",
    "* Subsequent layers build increasingly sophisticated representations of raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Representation](http://hugman.re.kr/blog/repr_learning)\n",
    "\n",
    "![표현학습의 예](http://hugman.re.kr/static/media/uploads/apple_rep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "숫자 덩어리(Tensor)의 장점\n",
    "1. 도메인의 전문가 없이도 쌓아놓은 데이터만 가지고도 정보를 구축할 수 있습니다.\n",
    "2. 어떠한 유형의 데이터든 하나로 묶을 수 있습니다.\n",
    "3. 사람의 기호로 표현하기 힘든 그 무엇(Feeling, Sense 등등 에 대한것을 표현?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런식의 Tensor들이 모여 컴퓨터 스스로 하나의 특징표현을 만들어내는것 = Representation Learning(표현학습)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Deep Learning learns layers of features](http://cfile22.uf.tistory.com/image/221020385709D5DA38487B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Deep Learning Apporoch](http://cfile22.uf.tistory.com/image/240D9C385709D5DE3AE44C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미리 공부하면 좋은 학습들\n",
    "    - 퍼셉트론\n",
    "    - 하강경사법 \n",
    "    - Overfitting\n",
    "    - 오류역전파 알고리즘\n",
    "    - 소벨마스크 \n",
    "    - 선형회귀/로지스틱 회귀 \n",
    "    - Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DNN에 관한 링크](http://sanghyukchun.github.io/75/)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modeler doesn't need to specify the interactions\n",
    "* When you train the model, the neural network gets weights that find the relevant patterns to make better predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The need for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A baseline neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차를 줄이는방법(시작한다는 말)<br> \n",
    "Error = Predicted - Actual <br>\n",
    "$$\\hat y - y$$\n",
    "<br>\n",
    "잔차가 작을 수록 좋은 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions wit multiple points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Making accurate predictions gets harder with more points\n",
    "* At any set of weights, there are many value of the error\n",
    "* ...corresponding to the many points we make prediction for\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aggregates errors in predictions from many data points into single number\n",
    "* measure of model's predictive performance<br>\n",
    "ex)<br>\n",
    "<br>\n",
    "    ### Squared error loss function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Prediction|Actual|Error|Squared Error|\n",
    "|---|---|---|---|\n",
    "|10|20|-10|100|\n",
    "    |8|3|5|25|\n",
    "    |6|1|5|25|    \n",
    "    \n",
    "* Total Squared Error:150\n",
    "* Mean Squared Error:50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서\n",
    "* Lower loss function value means a better model\n",
    "* Goal:Find the weights that give the lowest value for the loss function\n",
    "* do Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent-1(기울기 하강)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차를 줄이기 위해 기울기(**미분**)를 이용하여 점진적으로 오차를 줄여가는 방식 이후 Back Propagation에 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Imagine you are in a pitch dark field\n",
    "* Want to find the lowest point\n",
    "* Feel the ground to see how it slopes\n",
    "* Take a small step downhill\n",
    "* Repeat until it is uphill in every direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent staps\n",
    "* start at random point\n",
    "* Until you are somewhere flat:\n",
    "    * Find the slope\n",
    "    * Take a step downhil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the slops is positive:<br>\n",
    "    * Going opposite the slope means moveing to lower numbers\n",
    "    * Subtract the slope from the current value\n",
    "    * Too big a step might lead us astray\n",
    " \n",
    "Solution: learning rate\n",
    "    * Update each weight by subtracting\n",
    "$$Learning rate \\times slope$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slope calculation example\n",
    "\n",
    "$$x(3) \\rightarrow x(3) \\times w(2) \\rightarrow y(6)$$\n",
    "**Actual Target Value = 10**\n",
    "<br>\n",
    "<br>\n",
    "* To calculate the slope for a weight, need to multiply:\n",
    "    * Slope of the loss function w.r.t value at the node we feed into\n",
    "    * The value of the node that feeds into our weight\n",
    "    * Slope of the activation function w.r.t value we feed into"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W.R.T value???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Slope of the loss function w.r.t value at the node we feed into\n",
    "<br>\n",
    "$$2 \\times (predicted Value - Actual Value) = 2 \\times Error$$\n",
    "$$ 2 \\times -4 $$\n",
    "\n",
    "2. The value of the node that feeds into our weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slope calculation example\n",
    "\n",
    "$$x(3) \\rightarrow x(3) \\times w(2) \\rightarrow y(6)$$\n",
    "\n",
    "* slope of mean squared loss funcion\n",
    "$$= 2 \\times -4 \\times 3 $$\n",
    "$$-24$$\n",
    "\n",
    "* if learning rate is 0.01, the new weight would be\n",
    "\n",
    "$$2 - 0.01(-24) = 2.24$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to calculate slopes and update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds =  11\n",
      "error =  5\n",
      "gradient =  [30 40]\n",
      "error update =  2.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "weights = np.array([1, 2])\n",
    "\n",
    "input_data = np.array([3, 4])\n",
    "\n",
    "target = 6\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "preds = (weights * input_data).sum()\n",
    "\n",
    "print(\"preds = \",preds)\n",
    "\n",
    "error = preds - target\n",
    "\n",
    "print(\"error = \",error)\n",
    "\n",
    "# 여기서 부터 slope calculate\n",
    "\n",
    "# 왜 2를 곱하는가? 잘모르겠다..\n",
    "gradient = 2 * input_data * error\n",
    "print(\"gradient = \",gradient)\n",
    "\n",
    "# 가중치 부분 **중요**\n",
    "weights_updated = weights - learning_rate * gradient\n",
    "\n",
    "preds_updated = (weights_updated * input_data).sum()\n",
    "\n",
    "error_updated = preds_updated - target\n",
    "\n",
    "print(\"error update = \",error_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing a model with a single weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Optimizing 을 위해 gradient이용한다.](http://aikorea.org/cs231n/optimization-1)\n",
    "<br>\n",
    "기본원리는 기울기가 낮은 쪽으로 연속적으로 이속시켜 값이 최소가 되는 점(극값)에 다다르게 하는 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y - \\hat y$ 이렇게 나온 값으로 가중치에 변화를 주는 방식(learning_rate 도 같이 이용) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back propagation(역전파)](http://llnntms.tistory.com/31) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back propagation은 input과 output을 알고 있는 상태에서(이를 Supervised learning이라 함) 신경망을 학습 시기키는 방법이다.\n",
    "<br>\n",
    "<br>\n",
    "역전파 알고리즘을 적용시키기 이전에 MLP에 대해 몇가지 알아야할 것들이 있다.<br>\n",
    "<br>\n",
    " a. 초기 가중치, weight값은 랜덤으로 주어진다.<br>\n",
    "<br>\n",
    " b. 각각 노드, node는 하나의 퍼셉트론으로 생각한다. <br>\n",
    "    즉, 노드를 지나칠 때마다 활성함수를 적용한다.<br> \n",
    "    활성함수를 적용하기 이전을 net, 이후를 out이라고 하겠다.<br> \n",
    "    다음 레이어의 계산에는 out값을 사용한다. 마지막 out이 output이 된다.<br>\n",
    "<br>\n",
    " c. 활성함수는 시그모이드 sigmoid 함수로 한다.(시그모이드 함수에 관해서는 http://astralworld58.tistory.com/62)\n",
    "<br>\n",
    "<br>\n",
    "    (미분하기 용이해서 대표적으로 쓰이는 함수)\n",
    "![model](http://img1.daumcdn.net/thumb/R1920x0/?fname=http%3A%2F%2Fcfile1.uf.tistory.com%2Fimage%2F2674D73C586F43C229902C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "우리가 결과값으로 얻기를 바라는 값을 target, 실제로 얻은 결과 값을 output이라 하면, E는 오차이다.\n",
    "<br>\n",
    "![Back Propagartion 식](http://cfile25.uf.tistory.com/image/273A59455870564421B210)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 합($\\sum$)의 의미는 모든 output($E_{o1}, E_{o2}, \\ldots$)에서 발생한 오차를 모두 더 해주는것.<br>\n",
    "**최종 목적은 이 오차에 관한 함수**($E_{total}$)**의 함수값을 0에 가깝게 만드는것**이다.<br>\n",
    "오차가 0에 가까워지면, 신경망은 학습에 사용된 input들과 그에 유사한 input에 대해서 우리가 원하는 output,<br> \n",
    "정답이라고 할 수 있는 값들을 산출하는 것이 목표.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "![W1](http://img1.daumcdn.net/thumb/R1920x0/?fname=http%3A%2F%2Fcfile29.uf.tistory.com%2Fimage%2F2423C84C587240B21F45A9)\n",
    "<center>가중치 1개의 오차 E의 도식화. y=0일 때, E(오차)가 최소화</center>\n",
    "<br>\n",
    "![W2](http://img1.daumcdn.net/thumb/R1920x0/?fname=http%3A%2F%2Fcfile8.uf.tistory.com%2Fimage%2F2319F94C587240B22B7C9A)\n",
    "<center>가중치 2개의 오차 E의 도식화. w1=0, w2=0 일 때, E가 최소화</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpreopagation\n",
    "* Allows gradient descent to update all weights in neural network (by getting gradients for all weights)\n",
    "* comes from chain rule of calculus\n",
    "* Important to understand the process, but you will generally use a library that implements this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation precess - 1\n",
    "* Trying to estimate the slope of the loss function w.r.t each weight\n",
    "* do forward propagation to calculate predictions and errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Go back one layer at a time\n",
    "* Greadients for weight is product of:\n",
    "    1. Node value feeding into that weight\n",
    "    2. Slope of loss function w.r.t node it feeds into\n",
    "    3. Slope of activation function at the node it feeds into"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Activation Function\n",
    "<br>\n",
    "![ReLu](http://sefiks.com/wp-content/uploads/2017/08/relu-graph.png?w=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation process - 2\n",
    "* Need to also keep track of the slopes of the loss function w.r.t node values\n",
    "* Slope of node values are the sum of the slopes of all weights that come out of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in practice\n",
    "[Backpropagation in practice](https://m.blog.naver.com/PostView.nhn?blogId=samsjang&logNo=221033626685&proxyReferer=https%3A%2F%2Fwww.google.co.kr%2F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating slopes associated with any weight\n",
    "* Gradients for weight is product of:\n",
    "    1. Node value feeding into that weight\n",
    "    2. slope of activation function for the node being fed into\n",
    "    3. Slope of loss function w.r.t output node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation: Recap\n",
    "* Start at some random set of weights\n",
    "* Use forward propagation to make a prediction\n",
    "* Use backward propagation to calculate the slope of the loss function w.r.t each weight\n",
    "* Keep going with that cycle until we get to a flat part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent\n",
    "* It is common to calculate slopes on only a subset of the data ('batch')\n",
    "* Use a differnt batch of data to calculate the next update\n",
    "* Start over from the beginning onece all data is used\n",
    "* Each time throught the training data is called an epoch\n",
    "* When slopes are calculated on one batch at a time: That call => stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Specify Architecture\n",
    "* Compile\n",
    "* Fit\n",
    "* Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data import 부분\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data load 부분\n",
    "predictors = np.loadtxt('predictors_data.csv', delimiter=',') #파일 로드와 delimiter 방식\n",
    "n_cols = predictors.shape[1] # preds = (weights * input_data).sum()??\n",
    "\n",
    "# modeling 부분\n",
    "model = Sequential() # 어떤 모델을 쓸지 정의\n",
    "model.add(Dense(100, activation='relu', input_shape = (n_cols,))) \n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling and fitting a model(다시)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why you need to compile your model\n",
    "* Specify the optimizer\n",
    "    * controls the learning rate\n",
    "    * Many options and mathematically complex\n",
    "    * \"Adam\" is usually a good choice\n",
    "* Loss function\n",
    "    * \"mean_squared_error\" common for regresiion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_cols = predictors.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(100, activation='relu', input_shape='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(100. activetion='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a model\n",
    "\n",
    "* Applying backpropagation and gradient descent with your data to update the weights\n",
    "* Scaling data begore fitting can ease optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_cols = predictors.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(90, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "카테고리에 해당하는 것을 나누는것<br>\n",
    "<br>\n",
    "<br>\n",
    "대표적으로 onehot encoding<br>\n",
    "[link](http://www.openwith.net/?p=617)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelvalidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[교차검증 혹은 유효검사](http://daryan.tistory.com/21)\n",
    "\n",
    "1. 데이터 50% : 50% 분할\n",
    "2. Leave one out cross validation (LOOCV)\n",
    "\n",
    "3. **k-fold**<br>\n",
    "![k_fold](http://cfile7.uf.tistory.com/image/24153F4F5883D16E021099)<br>\n",
    "<br>\n",
    "정해진 것은 없지만, 보통 10개를 하고, 필요에 따라서 더 많이 합니다.\n",
    "명심해야할 것은 K가 적어질수록 모델의 평가는 편중될수 밖에 없으며, 결과도 정확하지 않습니다.\n",
    "k가 높을수록 평가는 bias가 낮아지지만, 결과의 분산이 높을수 있습니다.\n",
    "(즉 k=100일 경우 정확도가 1~100점이라고 나올수 있으며, 이럴경우에는 \n",
    "50점의 정확도를 가진다고 표현할수 있으며,\n",
    "k=20일 경우 정확도가 25~75점이라고 나와서 , \n",
    "정확도가 50이라고 표현할수도 있습니다. \n",
    " 동일한 정확도이지만 k-100일 경우가 분산이 더 높은 결과임은 틀림없습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting and Underfitting(Model capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
