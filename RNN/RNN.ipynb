{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RNN-0](http://aikorea.org/blog/rnn-tutorial-1/)<br>\n",
    "[RNN-1](https://www.slideshare.net/ssuser5ac863/rnn-76606859)<br>\n",
    "[RNN-3](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)<br>\n",
    "[RNN-4](http://www.whydsp.org/280)<br>\n",
    "[RNN-5 슬라이드 show로 보면 좋음](http://slideplayer.com/slide/7478657/)<br>\n",
    "[RNN-1(LSTM)](https://brunch.co.kr/@chris-song/9)<br>\n",
    "[RNN-2(LSTM)](https://deeplearning4j.org/kr/lstm)<br>\n",
    "[NLP 란?](http://konlpy-ko.readthedocs.io/ko/v0.4.3/start/)<br>\n",
    "아래 내용은 상기 링크를 정리한 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keyword\n",
    "* Recurrent: occurring often or repeatedly, 되풀이 되는, 재발하는\n",
    "* Natural Language Processing(NLP, 자연어 처리): NLP는 텍스트에서 의미잇는 정보를 분석 추출하고 이해하는 일련의 기술 집합."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network(RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 이란?\n",
    "* 순차적인 정보를 처리한다.\n",
    "* 다양한 Natrual Language Processing(NLP, 자연어 처리)을 한다.\n",
    "* 연속적인 데이터를(Sequential Data)처리하는데 적합하다.\n",
    "* RNN이 Recurrent한 이유는 가중치를 sequence(시퀀스)의 모든 요소마다 적용하고, 출력 결과는 이전의 계산 결과에 영향을 받기 때문이다.\n",
    "* 다른 방식으로 설명하면 RNN은 현재 계산된 결과에 대한 \"메모리\"정보를 갖고 있고 그 메모리가 다음 계산에 영향을 준다는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN-1](http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림은 RNN의 네트워크 전체 시퀀스에 대해 그려놓은 그림이다.<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN의 모델 원리\n",
    "* RNN을 펼쳐서 생각\n",
    "예를 들어, 5개의 단어로 이루어진 문장미연, RNN에서 유닛은 한 단어당 하나의 layer로 이루어진 5-layer로 구성됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $x_t$는 시간 스텝(time step) $t$에서의 입력값이다.\n",
    "* $s_t$는 시간 스텝 $t$에서의 hidden state이다. \n",
    "    * 네트워크의 \"메모리\" 부분으로서, 이전 $s_t = f(Ux_t + Ws_{t-1})$ 시간 스텝의 hiddent state 값과 현재 시간 스텝의 입력값에 의해 계산된다.\n",
    "    * Activation function의 $f$는 보통 tanh나 ReLU가 사용되고, 첫 hidden state를 계산하기 위한 $s_{t-1}$은 보통 0으로 초기화시킨다.\n",
    "* $o_t$는 시간 스텝 $t$에서의 출력값이다. 예를 들어, 문장에서 다음 단어를 추측하고 싶다면 단어 수만큼의 차원의 확률 벡터가 될 것이다. $O_t = softmax(Vs_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hidden state $s_t$는 네트워크의 메모리라고 생각할 수 있다.<br>\n",
    "$s_t$는 과거의 시간 스텝들에서 일어난 일들에 대한 정보를 전부 담고 있고,<br>\n",
    "출력값 $O_t$는 오로지 현재 시간 스텝 $t$의 메모리만 의존한다.<br>\n",
    "하지만 실제 구현에서는 너무 먼 과거에 발생한 일들은 잘 저장하지 못한다.<br>\n",
    "* 각 layer마다의 output 파라미터 값들이 전부 다른 기존의 신경망 구조와 달리,<br>\n",
    "RNN는 모든 시간 스텝에 대해 파라미터 값을 전부 내장하고 있다. ($U,V,W$ 가중치를 저장하고 있다.)<br>\n",
    "* RNN이 각 스텝마다 입력 값만 다를 뿐 거의 같은 계산을 하고 있다는 것을 보여준다.<br>\n",
    "이로 인해 각 각의 스텝마다 학습해야 하는 파라미터 수를 줄여준다.<br>\n",
    "* RNN은 시간에 따른 순차적 스텝에 따라 출력값을 내지만 다른 경우도 있다.<br>\n",
    "* 예를 들어 긍정/부정에 대한 감정을 추측할때,\n",
    "    - 단어의 위치에 대한 추측값을 내지 않고 최종 추측값 하나만 파악하여 결과 값을 도출하는 것이 유용하다.\n",
    "    - 다시 말해 입력값이 시간에 따른 순차적 스텝을 따라야 하는것은 아니다.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN 구조](https://i.imgur.com/Q8zv6TQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
