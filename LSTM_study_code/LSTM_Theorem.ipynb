{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ratsgo's blog](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)  \n",
    "[솔라리스의 인공지는 연구실](http://solarisailab.com/archives/1667)  \n",
    "[Chris송호연](https://brunch.co.kr/@chris-song/9)  \n",
    "하기 내용은 상기 링크를 정리한것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM White Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN(Recurrent Neural Networks)\n",
    "\n",
    "LSTM을 이해하기 위해서는 RNN을 알아야 한다.\n",
    "\n",
    "![](http://i.imgur.com/Q8zv6TQ.png)  \n",
    "그림1 http://i.imgur.com/Q8zv6TQ.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN은 히든 노드들이 방향성을 가진 엣지로 연결된 순환구조를 이루는(directed cycle)의 인공신경망 종류의 하나이다.  \n",
    "음성, 문자 등 순차 또는 시계열 데이터 처리에 적합한 모델로 CNN(Convolutional Neural Networks)과 함께 인기 있는 알고리즘이다.  \n",
    "  \n",
    "위 그림에서도 알 수 있듯 시퀀스 길이에 관계없이 인풋과 아웃풋을 받아들일 수 있는 네트워크 구조이기 떄문에 필요에 따라 여러가지 구조를 만든다는 장점이 있다.\n",
    "\n",
    "![](http://i.imgur.com/s8nYcww.png)\n",
    "그림2 http://i.imgur.com/s8nYcww.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN의 기본구조는 위와 같다.  \n",
    "녹색박스는 히든 state를 의미한다.  \n",
    "빨간박스는 input $x$,  \n",
    "파란박스는 output $y$이다.  \n",
    "현재 상태의 히든 state $h_t$는 직전 시점의 히든 state $h_{t-1}$를 받아 갱신된다.  \n",
    "  \n",
    "현재 상태의 output $y_t$는 $h_t$를 전달받아 갱신되는 구조이다.  \n",
    "수식에서도 알 수 있듯 히든 state의 활성함수(activation function)은 비선형 함수인 하이퍼볼릭탄젠트(tanh)를 사용한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 활성화 함수를 **비선형함수**로 사용하는 이유  \n",
    "선형 함수인 $h(x) = cx$를 활성화 함수로 사용한 3층 네트워크를 떠올려 보자.  \n",
    "이를 식으로 나타내면 다음 같이 $y(x) = h(h(h(x)))$가 된다.  \n",
    "이 계산은 $y(x) = ax$와 똑같은 식이다.  \n",
    "$a = c^3$이라고 하면 된다.  \n",
    "즉, 히든 레이어가 없는 네트워크로 표현이 가능하다.  \n",
    "그래서 층을 쌓는 혜턱을 얻으려면 활성함수로 반드시 비선형함수를 사용해야한다.  \n",
    "-밑바닥부터 시작하는 딥러닝-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 동작 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM(Long Short-Term Memory models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN은 관련 정보와 그 정보를 사용하는 지정 사이 거리가 멀 경우 역전파시 그래디언트가 점차 줄어 학습 능력이 떨어진다.  \n",
    "이를 Vanishing Gradient Problem 이라한다.  \n",
    "\n",
    "\n",
    "![](http://solarisailab.com/wp-content/uploads/2017/06/vanishing_gradient_problem.png)\n",
    "그림 http://solarisailab.com/wp-content/uploads/2017/06/vanishing_gradient_problem.png  \n",
    "  \n",
    "위 그림을 보면, 시간 1에서 들어온 input은 처음 네트워크 학습에 영향을 미치지다가 점점 새로운 input이 들어오며, hidden layer에서 영향력을 잃어간다.  \n",
    "종국에는 시간 1에서 들어온 input은 네트워크 학습에 영향을 끼치지 못한다.  \n",
    "이를 경사도(Gradient)가 사라지는(Vanishing)현상으로 불리기도 한다.  \n",
    "\n",
    "위와 같은 문제를 극복하기 위해서 고안된것이 LSTM(Long Short-Term Memory Network)이다.  \n",
    "LSTM Networks는 RNNs의 Hidden Layer를 \n",
    "* Input Gate, \n",
    "* Output Gate, \n",
    "* Forget Gate  \n",
    "  \n",
    "라는 세가지 게이트로 구성된 Memory Block으로 대체한 구조이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 게이트를 이용하여 Vanishing Gradient Problem을 해결하는 방법은 아래와 같다.\n",
    "![](http://solarisailab.com/wp-content/uploads/2017/06/vanishing_gradient_problem_solved.png)\n",
    "그림 (http://solarisailab.com/wp-content/uploads/2017/06/vanishing_gradient_problem_solved.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM은 RNN의 히든 state에 cell-state를 추가한 구조이다.  \n",
    "  \n",
    "![](http://i.imgur.com/jKodJ1u.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell state는 일종의 메모리 역활을 하게 되며 메모리에 저장된 데이터들 때문에 state가 오래 경과 되더라도 그래디언트가 비교적 잘 전파된다.  \n",
    "LSTM cell 수식은 아래와 같다.  \n",
    "###### $\\odot$ : Hadamard product 행렬의 내적\n",
    "  \n",
    "$$\n",
    "f_t = \\sigma(W_{xh\\_f^{X_t}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가 키워드 : Grid Long Short-Term Memory, Gate Recurrent Unit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
