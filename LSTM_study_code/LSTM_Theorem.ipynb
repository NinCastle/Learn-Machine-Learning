{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ratsgo's blog](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)  \n",
    "[솔라리스의 인공지는 연구실](http://solarisailab.com/archives/1667)  \n",
    "[Chris송호연](https://brunch.co.kr/@chris-song/9)  \n",
    "하기 내용은 상기 링크를 정리한것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM White Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN(Recurrent Neural Networks)\n",
    "\n",
    "LSTM을 이해하기 위해서는 RNN을 알아야 한다.\n",
    "\n",
    "![](http://i.imgur.com/Q8zv6TQ.png)  \n",
    "그림1 http://i.imgur.com/Q8zv6TQ.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN은 히든 노드들이 방향성을 가진 엣지로 연결된 순환구조를 이루는(directed cycle)의 인공신경망 종류의 하나이다.  \n",
    "음성, 문자 등 순차 또는 시계열 데이터 처리에 적합한 모델로 CNN(Convolutional Neural Networks)과 함께 인기 있는 알고리즘이다.  \n",
    "  \n",
    "위 그림에서도 알 수 있듯 시퀀스 길이에 관계없이 인풋과 아웃풋을 받아들일 수 있는 네트워크 구조이기 떄문에 필요에 따라 여러가지 구조를 만든다는 장점이 있다.\n",
    "\n",
    "![](http://i.imgur.com/s8nYcww.png)  \n",
    "그림2 http://i.imgur.com/s8nYcww.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN의 기본구조는 위와 같다.  \n",
    "녹색박스는 hidden state를 의미한다.  \n",
    "빨간박스는 input $x$,  \n",
    "파란박스는 output $y$이다.  \n",
    "현재 상태의 hidden state $h_t$는 직전 시점의 hidden state $h_{t-1}$를 받아 갱신된다.  \n",
    "  \n",
    "현재 상태의 output $y_t$는 $h_t$를 전달받아 갱신되는 구조이다.  \n",
    "수식에서도 알 수 있듯 히든 state의 활성함수(activation function)은 비선형 함수인 하이퍼볼릭탄젠트(tanh)를 사용한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 활성화 함수를 **비선형함수**로 사용하는 이유  \n",
    "선형 함수인 $h(x) = cx$를 활성화 함수로 사용한 3층 네트워크를 떠올려 보자.  \n",
    "이를 식으로 나타내면 다음 같이 $y(x) = h(h(h(x)))$가 된다.  \n",
    "이 계산은 $y(x) = ax$와 똑같은 식이다.  \n",
    "$a = c^3$이라고 하면 된다.  \n",
    "즉, 히든 레이어가 없는 네트워크로 표현이 가능하다.  \n",
    "그래서 층을 쌓는 혜턱을 얻으려면 활성함수로 반드시 비선형함수를 사용해야한다.  \n",
    "-밑바닥부터 시작하는 딥러닝-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM(Long Short-Term Memory models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN은 관련 정보와 그 정보를 사용하는 지정 사이 거리가 멀 경우 역전파시 그래디언트가 점차 줄어 학습 능력이 떨어진다.  \n",
    "이를 Vanishing Gradient Problem 이라한다.  \n",
    "\n",
    "\n",
    "![](http://solarisailab.com/wp-content/uploads/2017/06/vanishing_gradient_problem.png)  \n",
    "그림 http://solarisailab.com/wp-content/uploads/2017/06/vanishing_gradient_problem.png  \n",
    "  \n",
    "위 그림을 보면, 시간 1에서 들어온 input은 처음 네트워크 학습에 영향을 미치지다가 점점 새로운 input이 들어오며, hidden layer에서 영향력을 잃어간다.  \n",
    "종국에는 시간 1에서 들어온 input은 네트워크 학습에 영향을 끼치지 못한다.  \n",
    "이를 경사도(Gradient)가 사라지는(Vanishing)현상으로 불리기도 한다.  \n",
    "\n",
    "위와 같은 문제를 극복하기 위해서 고안된것이 LSTM(Long Short-Term Memory Network)이다.  \n",
    "LSTM Networks는 RNNs의 Hidden Layer를 \n",
    "* Input Gate, \n",
    "* Output Gate, \n",
    "* Forget Gate  \n",
    "  \n",
    "라는 세가지 게이트로 구성된 Memory Block으로 대체한 구조이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 게이트를 이용하여 Vanishing Gradient Problem을 해결하는 방법은 아래와 같다.  \n",
    "![](http://solarisailab.com/wp-content/uploads/2017/06/vanishing_gradient_problem_solved.png)  \n",
    "그림 (http://solarisailab.com/wp-content/uploads/2017/06/vanishing_gradient_problem_solved.png)  \n",
    "\n",
    "위 그림에서 \n",
    "* $O $는 gate가 열려있음을 뜻하고,  \n",
    "* $-$는 gate가 닫혀 있음을 뜻한다.  \n",
    "  \n",
    "시간 1에서의 인풋데이터를 받은 이후에 \n",
    "input Gate를 닫아버려서 새로운 input을 받지 않고, \n",
    "Forget Gate를 열어놔서 시간 1에서의 인풋 데이터를 계속 전달 받으면,  \n",
    "시간 1에서의 input의 영향력을 계속 가져갈 수 있다.  \n",
    "마지막으로, Output Gate를 열고 닫으면서, 시간 1에서의 input data의 영향력을 반영하거나 반영하지 않을 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM은 RNN의 히든 state에 cell-state를 추가한 구조이다.  \n",
    "  \n",
    "![](http://i.imgur.com/jKodJ1u.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell state는 일종의 메모리 역활을 하게 되며 메모리에 저장된 데이터들 때문에 state가 오래 경과 되더라도 그래디언트가 비교적 잘 전파된다.  \n",
    "LSTM cell 수식은 아래와 같다.  \n",
    "###### $\\odot$ : Hadamard product 행렬의 내적\n",
    "  \n",
    "$$\n",
    "f_t = \\sigma(W_{xh\\_f} x_t+ W_{hh\\_f} h_{t-1} + b_{h\\_f}) \\\\\n",
    "i_t = \\sigma(W_{xh\\_i} x_t+ W_{hh\\_i} h_{t-1} + b_{h\\_i}) \\\\\n",
    "o_t = \\sigma(W_{xh\\_o} x_t+ W_{hh\\_o} h_{t-1} + b_{h\\_o}) \\\\\n",
    "g_t = tanh(W_{xh\\_g} x_t+ W_{hh\\_g} h_{t-1} + b_{h\\_g}) \\\\\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot g_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**forget gate** $f_t$는 '과거 정보를 잊기'를 위한 게이트이다.  \n",
    "$h_{t-1}$과 $x_t$를 받아 시그모이드 취해준 값이 바로 forget gate가 내보내는 값이 된다.  \n",
    "시그모이드 함수의 출력 범위는 0에서 1사이 값이 되기 때문에 그 값이 0이전 상태의 정보는 잊고 1이라면 이전 상태의 정보를 온전히 기억하게 된다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input gate $i_t \\odot g_t$는 '현재 정보를 기억하기'위한 게이트이다.  \n",
    "$h_{t-1}$과 $x_t$를 받아 시그모이드를 취하고,  \n",
    "또 같은 입력으로 하이퍼볼릭탄젠트를 취해준 다음 Hardamard product 연산을 한 값이 바로 input gate가 내보내는 값이 된다.  \n",
    "  \n",
    "![](http://i.imgur.com/MPb3OvZ.png)  \n",
    "  \n",
    "그림 (http://i.imgur.com/MPb3OvZ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM의 순전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://i.imgur.com/7Jk6szL.png)  \n",
    "  \n",
    "위 그림에서 주목해야 할 점은 $H_t$이다.  \n",
    "이 행렬을 행 기준으로 4등분해, $i, f, o, g$ 각각에 해당하는 활성화 함수를 적용하는 방식으로 $i, f, o, g$를 계산한다.(다른 방식을 사용해도 무방함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수를 적용하는 방식은 아래의 그림과 같다.  \n",
    "  \n",
    "![](http://i.imgur.com/73zzDsC.png)  \n",
    "  \n",
    "그림 (http://i.imgur.com/73zzDsC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM의 역전파\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$df_t, di_t, dg_t, do_t$를 구하기까지 backward pass는 RNN과 유사하다.  \n",
    "  \n",
    "![](http://i.imgur.com/hTPFF4A.png)  \n",
    "  \n",
    "$dH_t$를 구하는 과정이 LSTM backward pass의 핵심이며,   \n",
    "$H_t$는 $i_t, f_t, o_t, g_t$로 구성된 행렬이다.  \n",
    "다른 말로 각각에 해당하는 그래디언트들을 합치면 $dH_t$를 만들 수 있다는 뜻이다.  \n",
    "$i,f,o$의 활성함수는 시그모이드고, $g$만 하이퍼볼릭탄젠트이다.  \n",
    "각각의 활성화 함수에 대한 로컬 그래디언트를 구해 들어온 그래디언틀를 곱해주면된다.  \n",
    "  \n",
    "![](http://i.imgur.com/ZkYBqPq.png)  \n",
    "  \n",
    "그림(http://i.imgur.com/ZkYBqPq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순전파 과정에서 $H_t$를 4등분하여 $i_t, f_t, o_t, g_t$를 구했던 것 처럼, backward pass에서는 $d_i, d_f, d_o, d_g$를 다시 합쳐 $dH_t$를 만든다.  \n",
    "이렇게 구한 $dH_t$는 다시 RNN과 같은 방식으로 역전파가 되는 구조이다.  \n",
    "  \n",
    "![](http://i.imgur.com/sM4rlmr.png)  \n",
    "  \n",
    "LSTM은 cell state와 히든 state가 재귀적으ㅗㄹ 구해지는 네트워크이다.  \n",
    "따라서 cell state의 그래디언트와 히든 state의 그래디언트는 직전 시점의 그래디언트 값에 영향을 받는다.   \n",
    "이는 RNN또한 같다.  \n",
    "이를 역전파에 적용하여 다음 계산에 사용한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가 키워드 : Grid Long Short-Term Memory, Gate Recurrent Unit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
